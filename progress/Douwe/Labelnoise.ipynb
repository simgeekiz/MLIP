{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, Convolution2D\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, MaxPool2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(1-split)% training data, split% test data\n",
    "\"\"\"\n",
    "def load_mnist(split = 0.10):\n",
    "    train = pd.read_csv('train.csv')\n",
    "    X = train.as_matrix()[:,1::] # All columns expect the first (label) column\n",
    "    y = train.label.as_matrix() \n",
    "    \n",
    "    # Create train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=42)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_predict(model):\n",
    "    # Predict kaggle test data\n",
    "    test = pd.read_csv(\"test.csv\").as_matrix().reshape(-1,28,28,1)\n",
    "    pred = model.predict(kaggle_test)\n",
    "    decoded = np.round(pred * np.asmatrix(range(10)).transpose()).astype(int)\n",
    "    \n",
    "    # Save as csv\n",
    "    output = pd.DataFrame(decoded)\n",
    "    output.index += 1\n",
    "    output.index.name = 'ImageId'\n",
    "    output.to_csv(\"predictions.csv\", header=['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', input_shape=(28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation = \"softmax\", name = \"Output_Layer\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add label noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Uniform label swapping</b> (sampling without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each n labels add alpha noisy labels\n",
    "\"\"\"\n",
    "def labelswap(y_train, alpha):\n",
    "    lol = y_train.copy()\n",
    "    n = int(y_train.shape[0]/(alpha+1))\n",
    "    an = y_train.shape[0] - n\n",
    "    print(\"{} clean labels, {} noisy labels, alpha={}\".format(n,an,alpha))\n",
    "\n",
    "    ind = np.random.choice(range(y_train.shape[0]), an, replace=False) # select an random labels\n",
    "    shift = np.random.randint(1,9,an) # shift the label randomly by 1 to 9\n",
    "    y_train[ind] = (y_train[ind] + shift) % 10 # calculate the new labels\n",
    "\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12600 clean labels, 25200 noisy labels, alpha=2\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Add label noise\n",
    "alpha = 2\n",
    "y_train = labelswap(y_train,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dljva\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Normalize data to deal with covariate shift\n",
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "# Reshape to tensor (for CNN input in keras)\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "# One-hot encoding for softmax layer\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 830,954\n",
      "Trainable params: 830,826\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "37800/37800 [==============================] - 11s - loss: 2.3102 - acc: 0.1478 - val_loss: 2.1758 - val_acc: 0.4129\n",
      "Epoch 2/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.2537 - acc: 0.1878 - val_loss: 1.8642 - val_acc: 0.7924\n",
      "Epoch 3/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.2329 - acc: 0.2044 - val_loss: 1.7558 - val_acc: 0.8162\n",
      "Epoch 4/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.2138 - acc: 0.2190 - val_loss: 1.7464 - val_acc: 0.8693\n",
      "Epoch 5/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.2015 - acc: 0.2213 - val_loss: 1.6668 - val_acc: 0.8824\n",
      "Epoch 6/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1845 - acc: 0.2369 - val_loss: 1.6422 - val_acc: 0.9343\n",
      "Epoch 7/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1773 - acc: 0.2414 - val_loss: 1.6014 - val_acc: 0.9176\n",
      "Epoch 8/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1687 - acc: 0.2481 - val_loss: 1.5951 - val_acc: 0.9421\n",
      "Epoch 9/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1635 - acc: 0.2508 - val_loss: 1.6567 - val_acc: 0.9445\n",
      "Epoch 10/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1577 - acc: 0.2563 - val_loss: 1.5013 - val_acc: 0.9567\n",
      "Epoch 11/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1477 - acc: 0.2644 - val_loss: 1.5502 - val_acc: 0.9498\n",
      "Epoch 12/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1464 - acc: 0.2664 - val_loss: 1.5782 - val_acc: 0.9560\n",
      "Epoch 13/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1412 - acc: 0.2731 - val_loss: 1.4797 - val_acc: 0.9526\n",
      "Epoch 14/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1365 - acc: 0.2761 - val_loss: 1.4260 - val_acc: 0.9564\n",
      "Epoch 15/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1322 - acc: 0.2774 - val_loss: 1.5364 - val_acc: 0.9495\n",
      "Epoch 16/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1268 - acc: 0.2810 - val_loss: 1.4545 - val_acc: 0.9579\n",
      "Epoch 17/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1213 - acc: 0.2881 - val_loss: 1.4795 - val_acc: 0.9657\n",
      "Epoch 18/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1168 - acc: 0.2907 - val_loss: 1.4745 - val_acc: 0.9640\n",
      "Epoch 19/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1126 - acc: 0.2921 - val_loss: 1.4359 - val_acc: 0.9626\n",
      "Epoch 20/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1081 - acc: 0.2954 - val_loss: 1.3948 - val_acc: 0.9633\n",
      "Epoch 21/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1052 - acc: 0.2968 - val_loss: 1.4610 - val_acc: 0.9602\n",
      "Epoch 22/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.1020 - acc: 0.2980 - val_loss: 1.5101 - val_acc: 0.9629\n",
      "Epoch 23/100\n",
      "37800/37800 [==============================] - 10s - loss: 2.0989 - acc: 0.3016 - val_loss: 1.3805 - val_acc: 0.9650\n",
      "Epoch 24/100\n",
      "37800/37800 [==============================] - 1239s - loss: 2.0901 - acc: 0.3029 - val_loss: 1.4133 - val_acc: 0.9657\n",
      "Epoch 25/100\n",
      "37800/37800 [==============================] - 16s - loss: 2.0869 - acc: 0.3039 - val_loss: 1.4609 - val_acc: 0.9660\n",
      "Epoch 26/100\n",
      "37800/37800 [==============================] - 12s - loss: 2.0848 - acc: 0.3043 - val_loss: 1.2970 - val_acc: 0.9657\n",
      "Epoch 27/100\n",
      "37800/37800 [==============================] - 12s - loss: 2.0812 - acc: 0.3054 - val_loss: 1.4386 - val_acc: 0.9626\n",
      "Epoch 28/100\n",
      "37800/37800 [==============================] - 12s - loss: 2.0797 - acc: 0.3034 - val_loss: 1.4297 - val_acc: 0.9579\n",
      "Epoch 29/100\n",
      "37800/37800 [==============================] - 12s - loss: 2.0753 - acc: 0.3077 - val_loss: 1.4733 - val_acc: 0.9581\n",
      "Epoch 30/100\n",
      "37800/37800 [==============================] - 12s - loss: 2.0759 - acc: 0.3074 - val_loss: 1.3528 - val_acc: 0.9595\n",
      "Epoch 31/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0713 - acc: 0.3080 - val_loss: 1.3061 - val_acc: 0.9612\n",
      "Epoch 32/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0674 - acc: 0.3056 - val_loss: 1.4048 - val_acc: 0.9614\n",
      "Epoch 33/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0618 - acc: 0.3084 - val_loss: 1.2060 - val_acc: 0.9600\n",
      "Epoch 34/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0581 - acc: 0.3102 - val_loss: 1.4791 - val_acc: 0.9519\n",
      "Epoch 35/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0570 - acc: 0.3088 - val_loss: 1.2175 - val_acc: 0.9552\n",
      "Epoch 36/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0549 - acc: 0.3113 - val_loss: 1.2840 - val_acc: 0.9564\n",
      "Epoch 37/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0528 - acc: 0.3110 - val_loss: 1.0118 - val_acc: 0.9474\n",
      "Epoch 38/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0503 - acc: 0.3103 - val_loss: 1.4932 - val_acc: 0.9510\n",
      "Epoch 39/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0493 - acc: 0.3119 - val_loss: 1.5034 - val_acc: 0.9545\n",
      "Epoch 40/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0462 - acc: 0.3117 - val_loss: 1.3048 - val_acc: 0.9545\n",
      "Epoch 41/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0422 - acc: 0.3141 - val_loss: 1.3893 - val_acc: 0.9607\n",
      "Epoch 42/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0410 - acc: 0.3149 - val_loss: 1.4326 - val_acc: 0.9598\n",
      "Epoch 43/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0427 - acc: 0.3145 - val_loss: 1.4242 - val_acc: 0.9460\n",
      "Epoch 44/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0382 - acc: 0.3146 - val_loss: 1.4423 - val_acc: 0.9457\n",
      "Epoch 45/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0389 - acc: 0.3154 - val_loss: 1.5188 - val_acc: 0.9405\n",
      "Epoch 46/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0349 - acc: 0.3154 - val_loss: 1.1354 - val_acc: 0.9455\n",
      "Epoch 47/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0342 - acc: 0.3151 - val_loss: 1.3846 - val_acc: 0.9571\n",
      "Epoch 48/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0280 - acc: 0.3168 - val_loss: 1.6004 - val_acc: 0.9388\n",
      "Epoch 49/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0267 - acc: 0.3166 - val_loss: 1.1591 - val_acc: 0.9483\n",
      "Epoch 50/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0251 - acc: 0.3149 - val_loss: 1.6005 - val_acc: 0.9250\n",
      "Epoch 51/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0276 - acc: 0.3174 - val_loss: 1.8047 - val_acc: 0.9024\n",
      "Epoch 52/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0198 - acc: 0.3188 - val_loss: 1.4028 - val_acc: 0.9407\n",
      "Epoch 53/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0177 - acc: 0.3187 - val_loss: 1.0734 - val_acc: 0.9498\n",
      "Epoch 54/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0207 - acc: 0.3177 - val_loss: 1.4549 - val_acc: 0.9326\n",
      "Epoch 55/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0154 - acc: 0.3171 - val_loss: 0.9638 - val_acc: 0.9255\n",
      "Epoch 56/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0171 - acc: 0.3164 - val_loss: 1.3693 - val_acc: 0.9381\n",
      "Epoch 57/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0110 - acc: 0.3187 - val_loss: 1.2302 - val_acc: 0.9352\n",
      "Epoch 58/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0096 - acc: 0.3176 - val_loss: 1.6527 - val_acc: 0.9300\n",
      "Epoch 59/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0075 - acc: 0.3212 - val_loss: 1.3622 - val_acc: 0.9288\n",
      "Epoch 60/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0037 - acc: 0.3184 - val_loss: 1.3240 - val_acc: 0.9376\n",
      "Epoch 61/100\n",
      "37800/37800 [==============================] - 13s - loss: 2.0017 - acc: 0.3207 - val_loss: 0.9486 - val_acc: 0.9243\n",
      "Epoch 62/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9976 - acc: 0.3196 - val_loss: 1.4913 - val_acc: 0.9348\n",
      "Epoch 63/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9974 - acc: 0.3214 - val_loss: 1.3039 - val_acc: 0.9350\n",
      "Epoch 64/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9975 - acc: 0.3204 - val_loss: 0.9352 - val_acc: 0.9181\n",
      "Epoch 65/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9956 - acc: 0.3208 - val_loss: 1.3537 - val_acc: 0.9350\n",
      "Epoch 66/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9885 - acc: 0.3223 - val_loss: 1.4932 - val_acc: 0.9217\n",
      "Epoch 67/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9894 - acc: 0.3244 - val_loss: 1.3007 - val_acc: 0.9440\n",
      "Epoch 68/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9899 - acc: 0.3230 - val_loss: 1.5656 - val_acc: 0.9143\n",
      "Epoch 69/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9852 - acc: 0.3230 - val_loss: 1.6196 - val_acc: 0.9024\n",
      "Epoch 70/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9837 - acc: 0.3233 - val_loss: 1.4980 - val_acc: 0.9274\n",
      "Epoch 71/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9820 - acc: 0.3233 - val_loss: 1.7035 - val_acc: 0.8540\n",
      "Epoch 72/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9740 - acc: 0.3234 - val_loss: 1.1617 - val_acc: 0.9250\n",
      "Epoch 73/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9798 - acc: 0.3241 - val_loss: 0.9086 - val_acc: 0.9093\n",
      "Epoch 74/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9743 - acc: 0.3246 - val_loss: 2.0483 - val_acc: 0.7014\n",
      "Epoch 75/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9688 - acc: 0.3259 - val_loss: 1.0440 - val_acc: 0.9190\n",
      "Epoch 76/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9731 - acc: 0.3235 - val_loss: 1.3184 - val_acc: 0.9324\n",
      "Epoch 77/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9678 - acc: 0.3267 - val_loss: 1.5196 - val_acc: 0.8981\n",
      "Epoch 78/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9698 - acc: 0.3245 - val_loss: 1.6470 - val_acc: 0.8702\n",
      "Epoch 79/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9605 - acc: 0.3266 - val_loss: 1.1597 - val_acc: 0.9119\n",
      "Epoch 80/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9661 - acc: 0.3264 - val_loss: 1.7124 - val_acc: 0.8390\n",
      "Epoch 81/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9686 - acc: 0.3256 - val_loss: 1.7546 - val_acc: 0.8014\n",
      "Epoch 82/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9573 - acc: 0.3262 - val_loss: 0.9779 - val_acc: 0.8800\n",
      "Epoch 83/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9566 - acc: 0.3258 - val_loss: 1.3518 - val_acc: 0.9050\n",
      "Epoch 84/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9527 - acc: 0.3283 - val_loss: 1.7491 - val_acc: 0.8236\n",
      "Epoch 85/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9557 - acc: 0.3274 - val_loss: 1.1756 - val_acc: 0.9057\n",
      "Epoch 86/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9547 - acc: 0.3268 - val_loss: 1.5007 - val_acc: 0.8383\n",
      "Epoch 87/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9463 - acc: 0.3279 - val_loss: 1.3847 - val_acc: 0.9090\n",
      "Epoch 88/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9443 - acc: 0.3294 - val_loss: 1.8297 - val_acc: 0.8112\n",
      "Epoch 89/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9461 - acc: 0.3279 - val_loss: 0.9645 - val_acc: 0.8900\n",
      "Epoch 90/100\n",
      "37800/37800 [==============================] - 12s - loss: 1.9420 - acc: 0.3285 - val_loss: 1.4187 - val_acc: 0.8802\n",
      "Epoch 91/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9406 - acc: 0.3301 - val_loss: 1.0192 - val_acc: 0.9198\n",
      "Epoch 92/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9439 - acc: 0.3301 - val_loss: 0.9309 - val_acc: 0.8871\n",
      "Epoch 93/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9343 - acc: 0.3319 - val_loss: 1.6998 - val_acc: 0.8236\n",
      "Epoch 94/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9366 - acc: 0.3325 - val_loss: 0.8945 - val_acc: 0.8860\n",
      "Epoch 95/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9367 - acc: 0.3346 - val_loss: 1.2073 - val_acc: 0.9069\n",
      "Epoch 96/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9301 - acc: 0.3337 - val_loss: 1.6438 - val_acc: 0.8464\n",
      "Epoch 97/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9300 - acc: 0.3322 - val_loss: 1.1882 - val_acc: 0.9005\n",
      "Epoch 98/100\n",
      "37800/37800 [==============================] - 14s - loss: 1.9283 - acc: 0.3324 - val_loss: 1.4077 - val_acc: 0.8721\n",
      "Epoch 99/100\n",
      "37800/37800 [==============================] - 13s - loss: 1.9211 - acc: 0.3345 - val_loss: 0.9353 - val_acc: 0.8905\n",
      "Epoch 100/100\n",
      "37800/37800 [==============================] - 12s - loss: 1.9230 - acc: 0.3326 - val_loss: 1.0029 - val_acc: 0.8826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff5d25ca90>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\").as_matrix().reshape(-1,28,28,1)\n",
    "results = model.predict(test)\n",
    "results = np.argmax(results, axis=1)\n",
    "results = pd.Series(results, name='Label')\n",
    "submission = pd.concat([pd.Series(range(1,28001), name='ImageId'), results], axis=1)\n",
    "submission.to_csv(\"predictions/labelnoise_adadelta_bn_b128.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "alpha = 2, 12600 clean 25200 noisy labels\n",
    "\n",
    "<pre>\n",
    "optimizer     epochs     batch size      train acc   val acc   kaggle acc\n",
    "adadelta      100        128             0.4                   0.497\n",
    "adam          100        128             0.6         0.60      0.38\n",
    "adadelta      100        256             0.42        0.75      0.40\n",
    "adadelta      100        64              0.38        0.82      0.47\n",
    "adadelta                                             0.88      0.17\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
